{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f1fe9-af40-4a57-aff0-99313d722f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# This source code is licensed under the CC-by-NC licence,\n",
    "# found in the LICENSE_CELL_DINO_CODE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe8d7c4-995c-44b4-a8d1-97be2badce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_IMAGES_DIR = \"sample_images/\"  # path to directory with cell images.\n",
    "REPO_DIR=\"\" # path to the dinov2 repo.\n",
    "# Also define the urls of the pretrained models CPurl, SCurl, FOVurl used in the next cell. Instructions to get the models urls are in the README.md."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b2a71b-8fb2-4ec3-8cd1-21d6f770f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "cell_dino_vits8 = torch.hub.load(REPO_DIR, 'cell_dino_cp_vits8', source='local', pretrained_url=CPurl)\n",
    "cell_dino_vitl16_sc = torch.hub.load(REPO_DIR, 'cell_dino_hpa_vitl16', source='local', pretrained_url=SCurl)\n",
    "cell_dino_vitl16_fov = torch.hub.load(REPO_DIR, 'cell_dino_hpa_vitl16', source='local', pretrained_url=FOVurl)\n",
    "#channel_adaptive_dino_vitl16 = torch.hub.load(REPO_DIR, 'channel_adaptive_dino_vitl16', source='local', pretrained_url=CAurl)\n",
    "# cell_dino_vitl14 = torch.hub.load(REPO_DIR, 'cell_dino_hpa_vitl14', source='local', pretrained_url=HRurl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6899c21-c4c2-43d4-8cdc-f6cce2c3686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from dinov2.hub.cell_dino.backbones import cell_dino_hpa_vitl16, cell_dino_cp_vits8\n",
    "from functools import partial\n",
    "from dinov2.eval.utils import ModelWithIntermediateLayers\n",
    "\n",
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "class self_normalize(object):\n",
    "    def __call__(self, x):\n",
    "        x = x / 255\n",
    "        m = x.mean((-2, -1), keepdim=True)\n",
    "        s = x.std((-2, -1), unbiased=False, keepdim=True)\n",
    "        x -= m\n",
    "        x /= s + 1e-7\n",
    "        return x\n",
    "\n",
    "normalize = self_normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779ecbb7-3247-4b9e-9248-25c8cbd74d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Example inference on HPA-FoV dataset --------------------------\n",
    "\n",
    "# 1- Read one human protein atlas HPA-FoV image (4 channels)\n",
    "img = torchvision.io.read_image(SAMPLE_IMAGES_DIR + \"HPA_FoV_00070df0-bbc3-11e8-b2bc-ac1f6b6435d0.png\")\n",
    "\n",
    "# 2- Normalise image as it was done for training\n",
    "img_hpa_fov = img.unsqueeze(0).to(device=DEVICE)\n",
    "img_hpa_fov = normalize(img_hpa_fov)\n",
    "\n",
    "# 3- Load model\n",
    "cell_dino_model = cell_dino_vitl16_fov\n",
    "cell_dino_model.to(device=DEVICE)\n",
    "cell_dino_model.eval()\n",
    "\n",
    "# 4- Inference\n",
    "features = cell_dino_model(img_hpa_fov)\n",
    "print(features)\n",
    "\n",
    "# 5- [Optional] feature extractor as used for linear evaluation\n",
    "autocast_ctx = partial(torch.cuda.amp.autocast, enabled=True, dtype=torch.float)\n",
    "model_with_interm_layers = ModelWithIntermediateLayers(cell_dino_model, 4, autocast_ctx)\n",
    "features_with_interm_layers = model_with_interm_layers(img_hpa_fov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e67e67-5824-4135-8ca2-f8396cf95cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Example inference on cell painting data --------------------------\n",
    "\n",
    "# 1- Read one cell painting image (5 channels)\n",
    "img = torchvision.io.read_image(SAMPLE_IMAGES_DIR + \"CP_BBBC036_24277_a06_1_976@140x149.png\")\n",
    "img5_channels = torch.zeros([1, 5, 160, 160])\n",
    "for c in range(5):\n",
    "    img5_channels[0, c] = img[0, :, 160 * c : 160 * (c + 1)]\n",
    "img5_channels = img5_channels.to(device=DEVICE)\n",
    "\n",
    "# 2- Normalise image as it was done for training\n",
    "img5_channels = normalize(img5_channels)\n",
    "\n",
    "# 3- Load model\n",
    "cell_dino_model = cell_dino_vits8\n",
    "cell_dino_model.to(device=DEVICE)\n",
    "cell_dino_model.eval()\n",
    "\n",
    "# 4- Inference\n",
    "features = cell_dino_model(img5_channels)\n",
    "print(features[0,0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada26fcc-fd27-4dbf-983c-bbe06fe04b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Example inference on HPA single cell dataset --------------------------\n",
    "\n",
    "# Read one human protein atlas HPA single cell image (4 channels)\n",
    "img = torchvision.io.read_image(SAMPLE_IMAGES_DIR + \"HPA_single_cell_00285ce4-bba0-11e8-b2b9-ac1f6b6435d0_15.png\")\n",
    "\n",
    "# 2- Normalise image as it was done for training\n",
    "img_hpa = img.unsqueeze(0).to(device=DEVICE)\n",
    "img_hpa = normalize(img_hpa)\n",
    "\n",
    "# 3- Load model\n",
    "cell_dino_model = cell_dino_vitl16_sc\n",
    "cell_dino_model.to(device=DEVICE)\n",
    "cell_dino_model.eval()\n",
    "\n",
    "# 4- Inference\n",
    "features = cell_dino_model(img_hpa)\n",
    "print(features)\n",
    "\n",
    "torch.save(features.cpu(), \"sample_features_hpa.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mypy310env)",
   "language": "python",
   "name": "mypy310env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
